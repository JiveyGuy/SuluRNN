################################################
#      Honors Project 2018 Jason Ivey          #
################################################



from __future__ import print_function
from six.moves import cPickle
from six import text_type
from utils import TextLoader
from model import Model

import numpy as np
import tensorflow as tf

import sys
import os
import os.path
import time
import argparse


# arg = str(sys.argv[1])

##########################################################
#                                                        #
#               Constants     Region                     #
#                                                        #
##########################################################

DEBUG = True
data_dir = "data/tinyshakespeare"
save_dir = "save"
log_dir = "logs"
save_every = 1000
init_from = None
modelType = "lstm"
rnn_size = 128
num_layers = 2
seq_length = 50
batch_size = 50
num_epochs = 50
grad_clip = 5.
learning_rate  = 0.002
decay_rate = 0.97
output_keep_prob = 1.0
input_keep_prob = 1.0
# vocab_size = None

##########################################################
#                                                        #
#               Functionals   Region                     #
#                                                        #
##########################################################

# Std debug
def debug(text):
	if DEBUG :
		print(text)

# Timer object
class Timer: 
    #This will time events in milliseconds
	def __init__(self):
		self.startTime = 0.0
	def start(self):
		self.startTime = int(round(time.time() * 1000))
	def end(self):
		return str(int(round(time.time() * 1000)) - self.startTime )

# Character generation
def sample(save_dir, prime, n, sample):

	# Sample.py main method courtesy 
	# of Sherjil Ozair (Source: https://github.com/sherjilozair/char-rnn-tensorflow)
	# # # # # # # # # # # # 
	# The main goal of Sample.py is to take the model developed by
	# train.py and generate characters of n length, this method accepted a directory, 
	# priming text, n length, and a sample at default as arguments from the console
	# by making use of argparse, I have removed the argsparse method calls and have instead 
	# made those parameters passable. 

    # with open(os.path.join(save_dir, 'config.pkl'), 'rb') as f:
    #     saved_args = cPickle.load(f)
    with open(os.path.join(save_dir, 'chars_vocab.pkl'), 'rb') as f:
        chars, vocab = cPickle.load(f)
    #Use most frequent char if no prime is given
    if prime == '':
        prime = chars[0]

    data_loader = TextLoader(data_dir, batch_size, seq_length)
    vocab_size = data_loader.vocab_size

    model = Model(data_dir, save_dir, log_dir, save_every, init_from, modelType, rnn_size, num_layers, 
	seq_length,  batch_size, num_epochs, grad_clip, learning_rate, decay_rate, output_keep_prob, 
	input_keep_prob, vocab_size, training=False)

    with tf.Session() as sess:
        tf.global_variables_initializer().run()
        saver = tf.train.Saver(tf.global_variables())
        ckpt = tf.train.get_checkpoint_state(save_dir)
        if ckpt and ckpt.model_checkpoint_path:
            saver.restore(sess, ckpt.model_checkpoint_path)
            # print(model.sample(sess, chars, vocab, n, prime,
                               # sample).encode('utf-8'))
            text = model.sample(sess, chars, vocab, n, prime,
                                sample).encode('utf-8')
            fixedText = text.decode("utf-8", "replace")
            print(fixedText)

# Model creation
def train(data_dir, save_dir, log_dir, save_every, init_from, modelType, rnn_size, num_layers, 
	seq_length,  batch_size, num_epochs, grad_clip, learning_rate, decay_rate, output_keep_prob, 
	input_keep_prob ):
	
	# Train.py main method courtesy 
	# of Sherjil Ozair (Source: https://github.com/sherjilozair/char-rnn-tensorflow)
	# # # # # # # # # # # # 
	# The main goal of Train.py is to generate a tensor flow model in a dynamic manner
	# by using one of four possible nueral network models. This method is highly adaptable to any 
	# machine learning needs in regards to textual input in a utf-8 format. Again, as in sample.py,
	# I have changed this method to accept its settings as parameters rather than cli flags/arguments. 


    data_loader = TextLoader(data_dir, batch_size, seq_length)
    vocab_size = data_loader.vocab_size

    # check compatibility if training is continued from previously saved model
    if init_from is not None: # check if all necessary files exist
        assert os.path.isdir(init_from)," %s must be a a path" % init_from
        assert os.path.isfile(os.path.join(init_from,"config.pkl")),"config.pkl file does not exist in path %s"%init_from
        assert os.path.isfile(os.path.join(init_from,"chars_vocab.pkl")),"chars_vocab.pkl.pkl file does not exist in path %s" % init_from
        ckpt = tf.train.latest_checkpoint(init_from)
        assert ckpt, "No checkpoint found"

        # open old config and check if models are compatible
        with open(os.path.join(init_from, 'config.pkl'), 'rb') as f:
            saved_model_args = cPickle.load(f)
        need_be_same = ["model", "rnn_size", "num_layers", "seq_length"]
        for checkme in need_be_same:
            assert vars(saved_model_args)[checkme]==vars(args)[checkme],"Command line argument and saved model disagree on '%s' "%checkme

        # open saved vocab/dict and check if vocabs/dicts are compatible
        with open(os.path.join(init_from, 'chars_vocab.pkl'), 'rb') as f:
            saved_chars, saved_vocab = cPickle.load(f)
        assert saved_chars==data_loader.chars, "Data and loaded model disagree on character set!"
        assert saved_vocab==data_loader.vocab, "Data and loaded model disagree on dictionary mappings!"

    if not os.path.isdir(save_dir):
        os.makedirs(save_dir)
    # with open(os.path.join(save_dir, 'config.pkl'), 'wb') as f:
    #     cPickle.dump(args, f)
    with open(os.path.join(save_dir, 'chars_vocab.pkl'), 'wb') as f:
        cPickle.dump((data_loader.chars, data_loader.vocab), f)

    model = Model(data_dir, save_dir, log_dir, save_every, init_from, modelType, rnn_size, num_layers, 
	seq_length,  batch_size, num_epochs, grad_clip, learning_rate, decay_rate, output_keep_prob, 
	input_keep_prob, vocab_size)

    with tf.Session() as sess:
        # instrument for tensorboard
        summaries = tf.summary.merge_all()
        writer = tf.summary.FileWriter(
                os.path.join(log_dir, time.strftime("%Y-%m-%d-%H-%M-%S")))
        writer.add_graph(sess.graph)

        sess.run(tf.global_variables_initializer())
        saver = tf.train.Saver(tf.global_variables())
        # restore model
        if init_from is not None:
            saver.restore(sess, ckpt)
        for e in range(num_epochs):
            sess.run(tf.assign(model.lr,
                               learning_rate * (decay_rate ** e)))
            data_loader.reset_batch_pointer()
            state = sess.run(model.initial_state)
            for b in range(data_loader.num_batches):
                start = time.time()
                x, y = data_loader.next_batch()
                feed = {model.input_data: x, model.targets: y}
                for i, (c, h) in enumerate(model.initial_state):
                    feed[c] = state[i].c
                    feed[h] = state[i].h

                # instrument for tensorboard
                summ, train_loss, state, _ = sess.run([summaries, model.cost, model.final_state, model.train_op], feed)
                writer.add_summary(summ, e * data_loader.num_batches + b)

                end = time.time()
                print("{}/{} (epoch {}), train_loss = {:.3f}, time/batch = {:.3f}"
                      .format(e * data_loader.num_batches + b,
                              num_epochs * data_loader.num_batches,
                              e, train_loss, end - start))
                if (e * data_loader.num_batches + b) % save_every == 0\
                        or (e == num_epochs-1 and
                            b == data_loader.num_batches-1):
                    # save for the last result
                    checkpoint_path = os.path.join(save_dir, 'model.ckpt')
                    saver.save(sess, checkpoint_path,
                               global_step=e * data_loader.num_batches + b)
                    print("model saved to {}".format(checkpoint_path))

# Interaction with train
def interactTrain():

	# Due to the highly customizable parameters accepted by train.py it would be beneficial
	# interact with it by an interface that has set defaults and only accepts parameters 
	# important to our goals. 
	#
	# These defualts come from the defaults set by the args definition 
	#         at : https://github.com/sherjilozair/char-rnn-tensorflow/blob/master/train.py 

	train(data_dir, save_dir, log_dir, save_every, init_from, modelType, rnn_size, num_layers, 
	seq_length,  batch_size, num_epochs, grad_clip, learning_rate, decay_rate, output_keep_prob, 
	input_keep_prob)

# Interaction with sample
def interactSample():
	save_dir = "save"
	n = 500
	prime = u'', 
	sam = 1
	sample(save_dir, prime, n, sample)

##########################################################
#                                                        #
#                  Main Logic Region                     #
#                                                        #
##########################################################

debug("Attempting to interact with train (god help me) ")
# interactTrain()
interactSample()